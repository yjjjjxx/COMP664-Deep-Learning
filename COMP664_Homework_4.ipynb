{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Backpropagation for bias vectors (1 point)\n",
        "\n",
        "In class, we discussed a multilayer perceptron (neural network) whose layers were all \"dense\", i.e. the output $a^m \\in \\mathbb{R}^{N^m}$ of the $m$th layer is computed as \n",
        "\\begin{align*}\n",
        "z^m &= W^m a^{m - 1} + b^m \\\\\n",
        "a^m &= \\sigma^m(z^m)\n",
        "\\end{align*}\n",
        "where $W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ is the weight matrix, $b^m \\in \\mathbb{R}^{N^m}$ is the bias vector, and $\\sigma^m$ is the nonlinearity. We showed that \n",
        "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m} a^{m - 1 \\top}$$\n",
        "Show that\n",
        "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
        "Hint: The derivation is almost the same as for $W$."
      ],
      "metadata": {
        "id": "nkK2eaK1SiNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Since the bias is a constant with respect to the input:\n",
        "\n",
        "\n",
        "\n",
        " $${‚àÇz^m \\over {‚àÇb^m}} = 1$$\n",
        "\n",
        "\n",
        "*   Therefore, the gradient of the cost function $C$ with respect to the bias vector $b^m$ is:\n",
        "\n",
        "$${‚àÇùê∂ \\over ‚àÇb^m}={‚àÇùê∂ \\over ‚àÇz^m} \\cdot {‚àÇz^m \\over {‚àÇb^m}} = {‚àÇùê∂ \\over ‚àÇz^m}$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zyeW07_paGA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. MLP from scratch (3 points)\n",
        "\n",
        "Using numpy only, implement backward pass or a sigmoid MLP. Specifically, you will need to implement this functionality in the `train` function in the `SigmoidMLP` class below. You should write numpy code to populate the two lists `weight_gradients` and `bias_gradient`, where each entry in each list corresponds to the gradient for a weight matrix or bias vector for each layer. Then, when you run the code cell at the bottom of this notebook, the trained MLP should output (approximately) 0, 1, 1, 0, having learned the [XOR function](https://en.wikipedia.org/wiki/Exclusive_or). Please us a binary cross-entropy loss, i.e.\n",
        "$$C(a^L, y) = (y - 1)\\log(1 - a^L) - y\\log(a^L)$$\n",
        "\n",
        "*Note 1*: All layers in your model, including the last layer, will use the sigmoid cross-entropy function. Remember that \n",
        "$$\n",
        "\\frac{\\partial}{\\partial x}\\mathrm{sigmoid}(x) = \\mathrm{sigmoid}(x)(1 - \\mathrm{sigmoid}(x))$$\n",
        "\n",
        "*Note 2*: As we mentioned in class,\n",
        "$$\n",
        "\\frac{\\partial C}{\\partial z^L} = a^L - y\n",
        "$$"
      ],
      "metadata": {
        "id": "TerrZtS6T31p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, inputs, outputs):\n",
        "        # Initialize weight matrix and bias vector\n",
        "        # Getting the initialization right can be tricky, but for this problem\n",
        "        # simply drawing from a standard normal distribution should work.\n",
        "        self.weights = np.random.randn(outputs, inputs)\n",
        "        self.biases = np.random.randn(outputs, 1)\n",
        "    def __call__(self, X):\n",
        "        # Compute \\sigmoid(Wx + b)\n",
        "        return 1/(1 + np.exp(-(self.weights.dot(X) + self.biases)))"
      ],
      "metadata": {
        "id": "elPwXt0eYD_E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SigmoidMLP:\n",
        "\n",
        "    def __init__(self, layer_widths):\n",
        "        self.layers = []\n",
        "        for inputs, outputs in zip(layer_widths[:-1], layer_widths[1:]):\n",
        "            self.layers.append(Layer(inputs, outputs))\n",
        "    \n",
        "    def train(self, inputs, targets, learning_rate):\n",
        "        # Forward pass - compute each layer's output and store it for later use\n",
        "        layer_outputs = [inputs]\n",
        "        for layer in self.layers:\n",
        "            layer_outputs.append(layer(layer_outputs[-1]))\n",
        "        \n",
        "        # Backward pass to compute gradients\n",
        "        weight_gradients = []\n",
        "        bias_gradients = []\n",
        "        # Calculate the error delta for the output layer\n",
        "        delta = layer_outputs[-1] - targets\n",
        "        # Iterate over the layers in reverse order (from output layer to input layer)\n",
        "        for i in range(len(self.layers), 0, -1):\n",
        "          # Compute the weight gradient for the current layer\n",
        "          weight_gradients.append(np.dot(delta, layer_outputs[i-1].T))\n",
        "          \n",
        "          # Compute the bias gradient for the current layer\n",
        "          bias_gradients.append(np.mean(delta, axis=1).reshape(self.layers[i-1].biases.shape))\n",
        "\n",
        "          # Calculate the error delta for the current layer\n",
        "          delta = np.dot(self.layers[i-1].weights.T, delta) * layer_outputs[i-1] * (1 - layer_outputs[i-1])\n",
        "      \n",
        "        # Reverse the weight_gradients and bias_gradients lists to match the order of the layers\n",
        "        weight_gradients.reverse()\n",
        "        bias_gradients.reverse()\n",
        "\n",
        "        # Perform gradient descent by applying updates\n",
        "        for weight_gradient, bias_gradient, layer in zip(weight_gradients, bias_gradients, self.layers):\n",
        "            layer.weights -= weight_gradient * learning_rate\n",
        "            layer.biases -= bias_gradient * learning_rate\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        a = inputs\n",
        "        for layer in self.layers:\n",
        "            a = layer(a)\n",
        "        return a"
      ],
      "metadata": {
        "id": "W-v0EeRsV8m9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mlp(n_iterations, learning_rate):\n",
        "    mlp = SigmoidMLP([2, 2, 1])\n",
        "    inputs = np.array([[0, 1, 0, 1], \n",
        "                       [0, 0, 1, 1]])\n",
        "    targets = np.array([[0, 1, 1, 0]])\n",
        "    for _ in range(int(1e3)):\n",
        "        mlp.train(inputs, targets, learning_rate)\n",
        "    return mlp"
      ],
      "metadata": {
        "id": "w4desjDOYBhj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to change the n_iterations and learning_rate values\n",
        "# but these worked for me\n",
        "mlp = train_mlp(1000, 1.)\n",
        "# The following calls should result in (approximately) 0, 1, 1, 0\n",
        "# If the outputs are somewhat close, your training has succeeded!\n",
        "print(mlp(np.array([0, 0]).reshape(-1, 1)))\n",
        "print(mlp(np.array([0, 1]).reshape(-1, 1)))\n",
        "print(mlp(np.array([1, 0]).reshape(-1, 1)))\n",
        "print(mlp(np.array([1, 1]).reshape(-1, 1)))"
      ],
      "metadata": {
        "id": "yoAl7lW_WyNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca26f7b-1a8d-493d-e4b7-6c38b89554c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00278562]]\n",
            "[[0.99777872]]\n",
            "[[0.99368235]]\n",
            "[[0.00169365]]\n"
          ]
        }
      ]
    }
  ]
}